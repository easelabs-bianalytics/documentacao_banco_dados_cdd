{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Guia do Usu\u00e1rio","text":""},{"location":"#1-introducao","title":"1. Introdu\u00e7\u00e3o","text":"<ul> <li> <p>Vis\u00e3o Geral do Projeto:        A cria\u00e7\u00e3o deste projeto surgiu da necessidade de integrar e preservar dados hist\u00f3ricos, uma vez que o CDD-Mensal disponibiliza apenas os \u00faltimos dois anos de informa\u00e7\u00f5es at\u00e9 o m\u00eas anterior, sem atualiza\u00e7\u00f5es di\u00e1rias, enquanto o CDD-Di\u00e1rio\u00a0possui as atualiza\u00e7\u00f5es di\u00e1rias mas mant\u00e9m dados apenas dos \u00faltimos tr\u00eas meses. Dessa forma, n\u00e3o consegu\u00edamos acompanhar nossa performance tanto di\u00e1ria quanto hist\u00f3rica a partir de uma \u00fanica fonte de dados.</p> <p>Para solucionar esse problema, inicialmente extra\u00edmos uma carga completa do CDD-Mensal e, a partir disso, realizamos atualiza\u00e7\u00f5es incrementais di\u00e1rias com base no CDD-Di\u00e1rio. Esse processo \u00e9 aplicado tanto \u00e0s tabelas fato quanto \u00e0s dimens\u00f5es.</p> <p>Al\u00e9m das vendas registradas no CDD, tamb\u00e9m carregamos no banco de dados informa\u00e7\u00f5es adicionais, como extras, acesso e PBM. Outra fonte incorporada ao banco, mas que n\u00e3o \u00e9 atualizada diariamente, \u00e9 o TD, que recebe atualiza\u00e7\u00f5es apenas do 15\u00ba dia at\u00e9 o \u00faltimo dia do m\u00eas (per\u00edodo dentro do qual a Close-up atualiza a base).</p> <p>Em resumo, todas essas fontes de dados passam por um processo de Extra\u00e7\u00e3o (coleta dos dados da origem), Transforma\u00e7\u00e3o (ajustes necess\u00e1rios para garantir a padroniza\u00e7\u00e3o e integridade dos dados) e Carregamento (inser\u00e7\u00e3o no banco de destino), garantindo um fluxo cont\u00ednuo e estruturado para a an\u00e1lise de desempenho.</p> </li> <li> <p>Principais Funcionalidades: A estrutura do projeto conta com as seguintes funcionalidades: </p> <ul> <li>Carregamento inicial de dados: Primeira carga realizada e j\u00e1 executada</li> <li>Atualiza\u00e7\u00f5es di\u00e1rias</li> <li>Auditoria: Confer\u00eancia e valida\u00e7\u00e3o dos dados em rela\u00e7\u00e3o ao banco de origem</li> </ul> </li> <li> <p>P\u00fablico-Alvo: Esse guia se destina a equipe de Business Analytics e todos aqueles envolvidos no desenvolvimento da infra-estrutura (administradores de banco de dados, engenheiros de dados, desenvolvedores, etc.) de dados da Ease Labs.</p> </li> </ul>"},{"location":"#2-pre-requisitos","title":"2. Pr\u00e9-requisitos","text":"<ul> <li> <p>VSCode: Voc\u00ea pode usar ou IDE se preferir</p> </li> <li> <p>Git e GitHub:</p> <ul> <li>Voc\u00ea deve ter o Git instalado em sua m\u00e1quina. Instru\u00e7\u00f5es de instala\u00e7\u00e3o do Git aqui.</li> <li>Voc\u00ea tamb\u00e9m deve ter uma conta no GitHub. Instru\u00e7\u00f5es de cria\u00e7\u00e3o de conta no GitHub aqui.</li> <li>Se voc\u00ea for usu\u00e1rio Windows, recomendo esse v\u00eddeo: Youtube.</li> <li>Tutorial de Git e Github b\u00e1sico Ebook.</li> <li>Se voc\u00ea j\u00e1 \u00e9 usu\u00e1rio Git, recomendo o v\u00eddeo do Akita: Youtube.</li> </ul> </li> <li> <p>Pyenv: \u00c9 usado para gerenciar vers\u00f5es do Python. Instru\u00e7\u00f5es de instala\u00e7\u00e3o do Pyenv aqui. Vamos usar nesse projeto o Python 3.12.0. Para usu\u00e1rios Windows, \u00e9 recomendado assistirem esse tutorial Youtube.</p> </li> <li> <p>Poetry: Este projeto utiliza Poetry para gerenciamento de depend\u00eancias. Instru\u00e7\u00f5es de instala\u00e7\u00e3o do Poetry aqui.Se voc\u00ea \u00e9 usu\u00e1rio Windows, recomendo assistir esse v\u00eddeo: Youtube. Que instala o Python, Poetry e VSCode. Mas um simples comando PIP INSTALL POETRY j\u00e1 resolve.</p> </li> <li> <p>Permiss\u00f5es de Acesso: </p> <ul> <li>Banco de dados Ease Labs no Snowflake </li> <li>Sharepoint EaseLabs </li> <li>Banco de dados de destino</li> <li>Conta do gmail easelabs.analytics </li> </ul> </li> </ul>"},{"location":"#instalacao-e-configuracao","title":"Instala\u00e7\u00e3o e Configura\u00e7\u00e3o","text":"<p>S\u00f3 prossiga para o passo-a-passo abaixo caso j\u00e1 tenha configurado a autentica\u00e7\u00e3o SSH para acessar o GitHub, caso contr\u00e1rio consulte a documenta\u00e7\u00e3o de configura\u00e7\u00e3o aqui</p>"},{"location":"#1-clone-o-repositorio","title":"1. Clone o reposit\u00f3rio:","text":"<pre><code>git clone git@github.com:easelabs-bianalytics/atualizacao_banco_dados_cdd.git\ncd atualizacao_banco_dados_cdd\n</code></pre>"},{"location":"#2-configure-a-versao-correta-do-python-com-pyenv","title":"2. Configure a vers\u00e3o correta do Python com <code>pyenv</code>:","text":"<pre><code>pyenv install 3.12.0\npyenv local 3.12.0\n</code></pre>"},{"location":"#3-configurar-poetry-para-python-version-3120-e-ative-o-ambiente-virtual","title":"3. Configurar poetry para Python version 3.12.0 e ative o ambiente virtual:","text":"<pre><code>poetry env use 3.12.0\npoetry shell\n</code></pre>"},{"location":"#4-instale-as-dependencias-do-projeto","title":"4. Instale as dependencias do projeto:","text":"<pre><code>poetry install\n</code></pre>"},{"location":"#5-crie-um-arquivo-env-na-raiz-do-projeto-e-copie-e-cole-as-credenciais-de-login","title":"5. Crie um arquivo .env na raiz do projeto e copie e cole as credenciais de login","text":"<pre><code>SHARE_USER=usuario@easelabs.com.br\nSHARE_PASS=SuaSenha\n\nSENDER_EMAIL=seuemail@gmail.com\nPASSWORD_EMAIL=SuaSenha\n\nACCOUNT=SuaAccount\nUSER=SeuUser\nPASSWORD_SNOW=SuaSenha\nDATABASE=SeuDatabase\nWAREHOUSE=SuaWarehouse\nROLE=SeuRole\n\nRAILWAY_DB_USER=dbuser\nRAILWAY_DB_PASS=dbsenha\nRAILWAY_DB_HOST=dbhost\nRAILWAY_DB_PORT=dbport\nRAILWAY_DB_NAME=dbname\n</code></pre>"},{"location":"#7-execute-o-comando-de-execucao-da-pipeline-para-realizar-a-etl","title":"7. Execute o comando de execuc\u00e3o da pipeline para realizar a ETL:","text":"<pre><code>task run\n</code></pre>"},{"location":"#8-verifique-no-banco-se-as-tabelas-foram-atualizadas-corretamente","title":"8. Verifique no banco se as tabelas foram atualizadas corretamente.","text":""},{"location":"#4-processo-de-carregamento-de-dados","title":"4. Processo de Carregamento de Dados","text":""},{"location":"#carregamento-inicial-de-dados","title":"Carregamento Inicial de Dados","text":"<ul> <li>Consulte esta documenta\u00e7\u00e3o para detalhes sobre o carregamento inicial de dados.</li> </ul>"},{"location":"#atualizacoes-incrementais-diarias","title":"Atualiza\u00e7\u00f5es Incrementais Di\u00e1rias","text":"<ul> <li> <p>Extra\u00e7\u00e3o de Dados:   Os dados s\u00e3o extra\u00eddos da tabela Fato e dimens\u00f5es das schema CDDD e TD. Tamb\u00e9m extra\u00edmos dados do Sharepoint para vendas extras, acesso e PBM </p> </li> <li> <p>Transforma\u00e7\u00e3o dos Dados:   Ap\u00f3s a extra\u00e7\u00e3o (CDDD), os dados s\u00e3o carregados em um dataframe pandas para realizar as seguintes transforma\u00e7\u00f5es:</p> </li> <li>Renomea\u00e7\u00e3o de colunas</li> <li>Formata\u00e7\u00e3o dos tipos de dados</li> <li>Cria\u00e7\u00e3o de novas colunas:<ul> <li>Coluna <code>hash</code>: Cria\u00e7\u00e3o de uma constraint para evitar a inser\u00e7\u00e3o de dados duplicados.</li> <li>Coluna <code>created_at</code>: Registro de um timestamp com a data e hora em que o dado foi inserido no banco de dados.</li> </ul> </li> </ul>"},{"location":"#tratamento-de-duplicidade","title":"Tratamento de Duplicidade","text":"<ul> <li> <p>Quando dados s\u00e3o inseridos na plataforma, \u00e9 realizada uma verifica\u00e7\u00e3o para garantir que n\u00e3o haja registros duplicados. A verifica\u00e7\u00e3o \u00e9 feita com base no hash, um campo \u00fanico que identifica cada registro de maneira exclusiva.</p> </li> <li> <p>Se um registro com o mesmo hash j\u00e1 existir:</p> <ul> <li>Em vez de criar uma nova entrada, o sistema atualiza o registro existente com os novos valores, garantindo que o banco de dados sempre mantenha a vers\u00e3o mais atualizada dos dados.</li> </ul> </li> </ul> <p>Resumo:   Se o hash do dado j\u00e1 existir na tabela, o aplicativo n\u00e3o cria uma nova entrada; ele atualiza os valores dos campos correspondentes, evitando duplica\u00e7\u00f5es e mantendo o banco de dados organizado e sempre atualizado.</p> <ul> <li>Observa\u00e7\u00e3o:   Embora a l\u00f3gica de duplicidade tenha sido implementada para evitar registros repetidos, ainda \u00e9 poss\u00edvel que algumas duplicatas ocorram devido a fatores imprevistos. At\u00e9 o momento, as duplicatas t\u00eam sido a principal causa de manuten\u00e7\u00e3o no banco de dados, especialmente devido \u00e0 falta de padr\u00e3o nos registros da Closeup (fonte original).</li> </ul>"},{"location":"#atualizacao-de-arquivos-do-sharepoint","title":"Atualiza\u00e7\u00e3o de Arquivos do SharePoint","text":"<ul> <li>Os arquivos baixados do SharePoint s\u00e3o atualizados diariamente. Por\u00e9m, como o volume de dados \u00e9 baixo, n\u00e3o \u00e9 aplicada nenhuma l\u00f3gica de duplicidade nesses arquivos. Em vez disso, as tabelas s\u00e3o simplesmente substitu\u00eddas pelas vers\u00f5es mais recentes.</li> </ul>"},{"location":"#mais-detalhes","title":"Mais Detalhes","text":"<ul> <li>Para informa\u00e7\u00f5es adicionais sobre os aspectos t\u00e9cnicos das atualiza\u00e7\u00f5es di\u00e1rias, consulte a se\u00e7\u00e3o Pipeline nesta documenta\u00e7\u00e3o, come\u00e7ando por aqui.</li> </ul>"},{"location":"#5-execucao-do-codigo","title":"5. Execu\u00e7\u00e3o do C\u00f3digo","text":"<ul> <li>Fluxo de Execu\u00e7\u00e3o: Uma explica\u00e7\u00e3o detalhada de como rodar os scripts ou ferramentas que realizam a extra\u00e7\u00e3o, transforma\u00e7\u00e3o e carregamento dos dados.</li> <li>Agendamento: Caso as atualiza\u00e7\u00f5es sejam agendadas, explique como elas s\u00e3o automatizadas (por exemplo, usando cron jobs, Airflow ou um agendador de tarefas).</li> <li>Logs e Monitoramento: Forne\u00e7a orienta\u00e7\u00f5es sobre como o usu\u00e1rio pode monitorar o sucesso ou falha das atualiza\u00e7\u00f5es, incluindo localiza\u00e7\u00f5es de arquivos de log e c\u00f3digos de erro.</li> </ul>"},{"location":"#6-tratamento-de-erros-e-solucao-de-problemas","title":"6. Tratamento de Erros e Solu\u00e7\u00e3o de Problemas","text":"<ul> <li>Problemas Comuns: Liste erros t\u00edpicos ou problemas que podem ocorrer durante a extra\u00e7\u00e3o, transforma\u00e7\u00e3o ou carregamento de dados.</li> <li>Solu\u00e7\u00f5es e Contornos: Forne\u00e7a poss\u00edveis solu\u00e7\u00f5es para os problemas listados.</li> <li>Dicas de Depura\u00e7\u00e3o: Oriente os usu\u00e1rios sobre como depurar o processo, incluindo como verificar logs e reexecutar etapas espec\u00edficas.</li> </ul>"},{"location":"#7-manutencao-e-otimizacoes","title":"7. Manuten\u00e7\u00e3o e Otimiza\u00e7\u00f5es","text":"<ul> <li>Manuten\u00e7\u00e3o do Banco de Dados: Explique como realizar a manuten\u00e7\u00e3o regular do banco de dados, como otimizar \u00edndices, realizar vacuum ou purgar dados antigos.</li> <li>Otimiza\u00e7\u00f5es de Desempenho: Ofere\u00e7a dicas para melhorar o desempenho tanto do carregamento inicial quanto das atualiza\u00e7\u00f5es incrementais (por exemplo, processamento paralelo, estrat\u00e9gias de indexa\u00e7\u00e3o ou particionamento).</li> <li>Escalabilidade: Se aplic\u00e1vel, explique como escalar o sistema para conjuntos de dados maiores ou fontes de dados adicionais.</li> </ul>"},{"location":"#8-consideracoes-de-seguranca","title":"8. Considera\u00e7\u00f5es de Seguran\u00e7a","text":"<ul> <li>Seguran\u00e7a dos Dados: Discuta as etapas para garantir a seguran\u00e7a dos dados sens\u00edveis (por exemplo, criptografia, chamadas de API seguras).</li> <li>Controle de Acesso: Explique como gerenciar o acesso ao banco de dados e garantir que apenas usu\u00e1rios autorizados possam executar o processo de carregamento de dados.</li> <li>Chaves de API e Credenciais: Explique como armazenar e gerenciar com seguran\u00e7a chaves de API, senhas ou credenciais necess\u00e1rias para acessar as fontes de dados.</li> </ul>"},{"location":"#9-melhores-praticas","title":"9. Melhores Pr\u00e1ticas","text":"<ul> <li>Qualidade de C\u00f3digo: Forne\u00e7a recomenda\u00e7\u00f5es sobre como manter o c\u00f3digo limpo, leg\u00edvel e f\u00e1cil de manter.</li> <li>Estrat\u00e9gia de Backup: Recomende como fazer backup do banco de dados e dos dados antes e ap\u00f3s o carregamento inicial e durante as atualiza\u00e7\u00f5es incrementais.</li> <li>Testes: Encoraje a realiza\u00e7\u00e3o de testes tanto na l\u00f3gica de carregamento inicial quanto nas atualiza\u00e7\u00f5es incrementais. Sugira testes unit\u00e1rios, de integra\u00e7\u00e3o e verifica\u00e7\u00f5es de valida\u00e7\u00e3o de dados.</li> </ul>"},{"location":"auditoria/","title":"Auditoria de Discrep\u00e2ncias e Sincroniza\u00e7\u00e3o de Dados","text":""},{"location":"auditoria/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este documento descreve o processo de auditoria de discrep\u00e2ncias entre os bancos de dados de origem e destino. O objetivo \u00e9 identificar inconsist\u00eancias nos dados e, se necess\u00e1rio, sincronizar as informa\u00e7\u00f5es.</p>"},{"location":"auditoria/#configuracao-inicial","title":"Configura\u00e7\u00e3o Inicial","text":"<p>O c\u00f3digo carrega vari\u00e1veis de ambiente e configura os motores de banco de dados:</p> <ul> <li> <p><code>engine_origem</code>: Conex\u00e3o com o banco de origem.</p> </li> <li> <p><code>engine_destino</code>: Conex\u00e3o com o banco de destino.</p> </li> <li> <p>Vari\u00e1veis de ambiente:</p> <ul> <li><code>SENDER_EMAIL</code></li> <li><code>PASSWORD_EMAIL</code></li> </ul> </li> </ul> <p>O logging \u00e9 configurado para registrar os eventos da auditoria.</p>"},{"location":"auditoria/#estrutura-de-dados","title":"Estrutura de Dados","text":"<p>A tabela principal utilizada \u00e9 <code>fato_cdd</code>, localizada no esquema <code>cddd</code>. Os principais campos considerados s\u00e3o:</p> <ul> <li><code>cod_anomes</code>: Data no formato <code>YYYYMMDD</code>.</li> <li><code>und</code>: Unidade a ser somada.</li> </ul>"},{"location":"auditoria/#funcoes-implementadas","title":"Fun\u00e7\u00f5es Implementadas","text":""},{"location":"auditoria/#calcular_somas_mensais","title":"<code>calcular_somas_mensais</code>","text":"<p>Calcula as somas mensais da tabela no banco de origem ou destino.</p> <p>Par\u00e2metros:</p> <ul> <li> <p><code>engine</code>: Conex\u00e3o com o banco de dados.</p> </li> <li> <p><code>tabela</code>: Nome da tabela.</p> </li> <li> <p><code>data_col</code>: Coluna de data.</p> </li> <li> <p><code>unidade_col</code>: Coluna de unidades.</p> </li> <li> <p><code>origem_destino</code>: Define se os dados s\u00e3o da origem ou destino.</p> </li> </ul>"},{"location":"auditoria/#deletar_mes","title":"<code>deletar_mes</code>","text":"<p>Deleta os registros do m\u00eas no qual a discrep\u00e2ncia foi identificada na tabela de destino.</p> <p>Par\u00e2metros:</p> <ul> <li> <p><code>mes</code>: M\u00eas no formato <code>YYYY-MM</code>.</p> </li> <li> <p><code>engine_destino</code>: Conex\u00e3o com o banco de destino.</p> </li> <li> <p><code>tabela</code>: Nome da tabela.</p> </li> <li> <p><code>data_col</code>: Coluna de data.</p> </li> </ul>"},{"location":"auditoria/#enviar_email","title":"<code>enviar_email</code>","text":"<p>Envia notifica\u00e7\u00f5es por e-mail sobre discrep\u00e2ncias e sincroniza\u00e7\u00f5es.</p> <p>Par\u00e2metros:</p> <ul> <li> <p><code>subject</code>: Assunto do e-mail.</p> </li> <li> <p><code>body</code>: Corpo do e-mail.</p> </li> <li> <p><code>to_email</code>: Destinat\u00e1rio.</p> </li> </ul>"},{"location":"auditoria/#auditoria","title":"<code>auditoria</code>","text":"<p>Executa a auditoria entre os bancos de dados de origem e destino.</p> <p>Passos do processo:</p> <ol> <li> <p>Obt\u00e9m os totais mensais de unidades para origem e destino.</p> </li> <li> <p>Compara os valores e identifica discrep\u00e2ncias.</p> </li> <li> <p>Caso haja discrep\u00e2ncia:</p> <ul> <li>Registra no log.</li> <li>Sincroniza os dados se a data m\u00ednima do m\u00eas na origem for o primeiro dia.</li> <li>Envia notifica\u00e7\u00e3o por e-mail.</li> </ul> </li> </ol>"},{"location":"auditoria/#conclusao","title":"Conclus\u00e3o","text":"<p>Esse processo permite garantir a integridade dos dados entre os sistemas. Se discrep\u00e2ncias forem encontradas, a\u00e7\u00f5es corretivas s\u00e3o tomadas automaticamente.</p>"},{"location":"carregamento_dados/","title":"Em Constru\u00e7\u00e3o","text":""},{"location":"configuracao_github/","title":"Manual de Instru\u00e7\u00f5es: Clonando Reposit\u00f3rio e Configura\u00e7\u00e3o de Autentica\u00e7\u00e3o SSH","text":""},{"location":"configuracao_github/#objetivo","title":"Objetivo","text":"<p>Este manual ir\u00e1 orient\u00e1-lo na configura\u00e7\u00e3o da autentica\u00e7\u00e3o SSH para acessar o GitHub e na clonagem do reposit\u00f3rio remoto para come\u00e7ar a trabalhar no projeto localmente.</p>"},{"location":"configuracao_github/#passo-1-verificar-se-o-git-esta-instalado","title":"Passo 1: Verificar se o Git est\u00e1 instalado","text":"<p>Primeiro, verifique se o Git est\u00e1 instalado na sua m\u00e1quina. Abra o terminal e execute o comando:</p> <pre><code>git --version\n</code></pre> <p>Se o Git n\u00e3o estiver instalado, siga as instru\u00e7\u00f5es para instal\u00e1-lo no site oficial do Git.</p>"},{"location":"configuracao_github/#passo-2-configuracao-de-chave-ssh","title":"Passo 2: Configura\u00e7\u00e3o de chave SSH","text":"<p>Para configurar a autentica\u00e7\u00e3o SSH e garantir que voc\u00ea possa acessar o reposit\u00f3rio remoto no GitHub sem precisar inserir sua senha a cada vez, siga os seguintes passos:</p>"},{"location":"configuracao_github/#21-gerar-uma-nova-chave-ssh","title":"2.1: Gerar uma nova chave SSH","text":"<p>Execute o comando abaixo para gerar uma nova chave SSH. Substitua seu e-mail associado ao GitHub.</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"seu-email@dominio.com\"\n</code></pre> <p>Pressione Enter para aceitar o local padr\u00e3o para salvar a chave.</p>"},{"location":"configuracao_github/#22-adicionar-a-chave-ssh-ao-agente-ssh","title":"2.2: Adicionar a chave SSH ao agente SSH","text":"<p>Execute o seguinte comando para iniciar o agente SSH, caso ainda n\u00e3o esteja em execu\u00e7\u00e3o:</p> <pre><code>eval \"$(ssh-agent -s)\"\n</code></pre> <p>Adicione sua chave SSH privada ao agente com o comando abaixo:</p> <pre><code>ssh-add ~/.ssh/id_rsa\n</code></pre>"},{"location":"configuracao_github/#23-adicionar-chave-ssh-a-sua-conta-do-github","title":"2.3: Adicionar chave SSH \u00e0 sua conta do GitHub","text":"<p>Obtenha a chave SSH p\u00fablica com o comando:</p> <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre> <p>Copie o conte\u00fado gerado.</p> <p>Agora, acesse o GitHub:</p> <ol> <li>V\u00e1 para Settings (Configura\u00e7\u00f5es).</li> <li>Clique em SSH and GPG keys no menu \u00e0 esquerda.</li> <li>Clique em New SSH key.</li> <li>Cole sua chave SSH p\u00fablica copiada na caixa Key e d\u00ea um nome identific\u00e1vel.</li> <li>Clique em Add SSH key.</li> </ol>"},{"location":"configuracao_github/#passo-3-testar-a-conexao-ssh-com-o-github","title":"Passo 3: Testar a Conex\u00e3o SSH com o GitHub","text":"<p>Execute o comando abaixo para testar a conex\u00e3o com o GitHub:</p> <pre><code>ssh -T git@github.com\n</code></pre> <p>Se a conex\u00e3o for bem-sucedida, voc\u00ea ver\u00e1 uma mensagem como:</p> <pre><code>Hi username! You've successfully authenticated, but GitHub does not provide shell access.\n</code></pre>"},{"location":"configuracao_github/#passo-4-clonar-o-repositorio-remoto","title":"Passo 4: Clonar o Reposit\u00f3rio Remoto","text":"<p>Agora, com a autentica\u00e7\u00e3o SSH configurada, voc\u00ea pode clonar o reposit\u00f3rio do GitHub usando o m\u00e9todo SSH.</p> <p>No GitHub, acesse a p\u00e1gina do reposit\u00f3rio que deseja clonar, clique no bot\u00e3o Code e copie a URL de clonagem SSH.</p> <p>No terminal, execute o seguinte comando para clonar o reposit\u00f3rio:</p> <pre><code>git clone git@github.com:easelabs-bianalytics/atualizacao_banco_dados_cdd.git\n</code></pre>"},{"location":"configuracao_github/#passo-5-configuracao-do-repositorio-local","title":"Passo 5: Configura\u00e7\u00e3o do Reposit\u00f3rio Local","text":"<p>Ap\u00f3s clonar o reposit\u00f3rio, navegue at\u00e9 o diret\u00f3rio do reposit\u00f3rio clonado:</p> <pre><code>cd atualizacao_banco_dados_cdd\n</code></pre> <p>Agora, voc\u00ea pode come\u00e7ar a trabalhar no seu reposit\u00f3rio local.</p>"},{"location":"configuracao_github/#passo-6-sincronizando-alteracoes","title":"Passo 6: Sincronizando Altera\u00e7\u00f5es","text":""},{"location":"configuracao_github/#61-fazendo-alteracoes-locais","title":"6.1: Fazendo altera\u00e7\u00f5es locais","text":"<p>Sempre que voc\u00ea fizer altera\u00e7\u00f5es no seu reposit\u00f3rio local, execute os seguintes comandos para salvar e enviar as altera\u00e7\u00f5es para o reposit\u00f3rio remoto.</p> <ol> <li> <p>Adicionar altera\u00e7\u00f5es ao \u00edndice:     <code>git add .</code>     Isso adiciona todas as altera\u00e7\u00f5es ao \u00edndice para prepara\u00e7\u00e3o.</p> </li> <li> <p>Fazer commit das altera\u00e7\u00f5es:     <code>git commit -m \"Descri\u00e7\u00e3o do que foi alterado\"</code></p> </li> <li> <p>Enviar as altera\u00e7\u00f5es para o reposit\u00f3rio remoto:     <code>git push origin main</code></p> </li> </ol>"},{"location":"configuracao_github/#62-baixando-alteracoes-do-repositorio-remoto","title":"6.2: Baixando altera\u00e7\u00f5es do reposit\u00f3rio remoto","text":"<p>Se houver altera\u00e7\u00f5es no reposit\u00f3rio remoto que voc\u00ea ainda n\u00e3o tem no seu reposit\u00f3rio local, execute o comando abaixo para baixar essas altera\u00e7\u00f5es:</p> <pre><code>git pull origin main\n</code></pre> <p>Isso ir\u00e1 garantir que seu reposit\u00f3rio local esteja sempre atualizado com as altera\u00e7\u00f5es do reposit\u00f3rio remoto.</p> <p>Com esses passos, voc\u00ea estar\u00e1 pronto para interagir com seu reposit\u00f3rio remoto usando SSH e manter a sincroniza\u00e7\u00e3o entre as vers\u00f5es local e remota.</p>"},{"location":"dados/","title":"Estrutura da Pasta <code>dados</code>","text":"<p>A pasta <code>dados</code> \u00e9 organizada para armazenar e gerenciar diferentes tipos de arquivos relacionados \u00e0 execu\u00e7\u00e3o de processos de transforma\u00e7\u00e3o de dados e auditoria. Abaixo est\u00e3o as subpastas que fazem parte dessa estrutura:</p>"},{"location":"dados/#estrutura-das-pastas","title":"Estrutura das Pastas","text":""},{"location":"dados/#1-data","title":"1. data","text":"<p>A pasta <code>data</code> cont\u00e9m informa\u00e7\u00f5es relacionadas \u00e0 \u00faltima data de atualiza\u00e7\u00e3o dos dados na tabela fato <code>td</code>. Essa pasta \u00e9 usada para controlar a data da \u00faltima execu\u00e7\u00e3o e garantir que os dados mais recentes sejam processados corretamente.</p> <ul> <li>Fun\u00e7\u00e3o: Guarda a \u00faltima data dos dados da tabela fato.</li> <li>Uso: Utilizada para rastrear a \u00faltima data de extra\u00e7\u00e3o, garantindo que as transforma\u00e7\u00f5es posteriores utilizem os dados mais recentes.</li> </ul>"},{"location":"dados/#2-logs","title":"2. logs","text":"<p>A pasta <code>logs</code> \u00e9 respons\u00e1vel por armazenar os logs de execu\u00e7\u00e3o da auditoria. Cada execu\u00e7\u00e3o dos processos de auditoria gera logs que s\u00e3o armazenados nesta pasta para monitoramento e an\u00e1lise de falhas ou erros durante o processo de transforma\u00e7\u00e3o.</p> <ul> <li>Fun\u00e7\u00e3o: Armazena os logs de execu\u00e7\u00e3o da auditoria.</li> <li>Uso: Permite a an\u00e1lise de falhas e performance, ajudando na resolu\u00e7\u00e3o de problemas e no acompanhamento do processo de auditoria.</li> </ul>"},{"location":"dados/#3-pbm","title":"3. pbm","text":"<p>A pasta <code>pbm</code> \u00e9 dedicada ao armazenamento dos arquivos do PBM extra\u00eddos do SharePoint. Esses arquivos s\u00e3o usados para transforma\u00e7\u00f5es antes de serem carregados no banco de dados.</p> <ul> <li>Fun\u00e7\u00e3o: Armazena os arquivos do PBM extra\u00eddos do SharePoint.</li> <li>Uso: Serve para armazenar os arquivos que precisam passar por transforma\u00e7\u00e3o antes de serem enviados ao banco de dados.</li> </ul>"},{"location":"dados/#4-planilhas","title":"4. planilhas","text":"<p>A pasta <code>planilhas</code> guarda os arquivos relacionados a vendas extras, acessos e slow changing dimensions, tamb\u00e9m extra\u00eddos do SharePoint. Assim como a pasta <code>pbm</code>, esses arquivos s\u00e3o transformados antes de serem carregados no banco de dados.</p> <ul> <li>Fun\u00e7\u00e3o: Armazena os arquivos de vendas extras, acesso e slow changing dimensions extra\u00eddos do SharePoint.</li> <li>Uso: Processamento e transforma\u00e7\u00e3o desses arquivos antes de envi\u00e1-los ao banco de dados.</li> </ul>"},{"location":"dados/#5-ticket-medio","title":"5. ticket medio","text":"<p>A pasta <code>ticket medio</code> segue a mesma l\u00f3gica das pastas <code>pbm</code> e <code>planilhas</code>, mas \u00e9 focada em armazenar arquivos espec\u00edficos relacionados ao ticket m\u00e9dio. Esses arquivos tamb\u00e9m s\u00e3o extra\u00eddos do SharePoint, transformados e posteriormente enviados ao banco de dados.</p> <ul> <li>Fun\u00e7\u00e3o: Armazena os arquivos relacionados ao ticket m\u00e9dio extra\u00eddos do SharePoint.</li> <li>Uso: Processamento e transforma\u00e7\u00e3o dos arquivos relacionados a essa m\u00e9trica antes do carregamento no banco de dados.</li> </ul>"},{"location":"dados/#resumo-da-estrutura","title":"Resumo da Estrutura","text":"<p>A estrutura da pasta <code>dados</code> est\u00e1 organizada para facilitar o armazenamento e a transforma\u00e7\u00e3o de diferentes tipos de dados extra\u00eddos do SharePoint. Cada pasta tem uma fun\u00e7\u00e3o espec\u00edfica, seja para armazenar dados de auditoria, arquivos de PBM, ou arquivos relacionados a vendas e ticket m\u00e9dio. O processo de transforma\u00e7\u00e3o e envio para o banco de dados ocorre ap\u00f3s o armazenamento nesses diret\u00f3rios.</p>"},{"location":"funcoes_cdd/","title":"Fun\u00e7\u00f5es","text":"<p>As fun\u00e7\u00f5es presentes neste script s\u00e3o respons\u00e1veis por executar opera\u00e7\u00f5es de integra\u00e7\u00e3o e processamento de dados, abrangendo tarefas como manipula\u00e7\u00e3o de arquivos e intera\u00e7\u00e3o com bancos de dados. Elas s\u00e3o amplamente utilizadas em diversas partes do projeto e foram desenvolvidas com o objetivo de eliminar redund\u00e2ncias no c\u00f3digo, promovendo maior reutiliza\u00e7\u00e3o e organiza\u00e7\u00e3o.</p>"},{"location":"funcoes_cdd/#estrutura-do-script","title":"Estrutura do Script","text":""},{"location":"funcoes_cdd/#1-bibliotecas-importadas","title":"1. Bibliotecas Importadas","text":"<p>O script faz uso de v\u00e1rias bibliotecas para gerenciar as opera\u00e7\u00f5es de banco de dados e manipula\u00e7\u00e3o de dados:</p> <ul> <li><code>os</code>: Para opera\u00e7\u00f5es de sistema de arquivos.</li> <li><code>hashlib</code>: Para gerar hashes SHA-256.</li> <li><code>datetime</code>: Para trabalhar com datas e hor\u00e1rios.</li> <li><code>pandas</code>: Para manipula\u00e7\u00e3o de dados e leitura de arquivos.</li> <li><code>sqlalchemy</code>: Para interagir com bancos de dados SQL.</li> <li><code>snowflake.sqlalchemy</code>: Para conectar ao Snowflake.</li> <li><code>dotenv</code>: Para carregar vari\u00e1veis de ambiente a partir de um arquivo <code>.env</code>.</li> </ul>"},{"location":"funcoes_cdd/#2-funcoes-definidas","title":"2. Fun\u00e7\u00f5es Definidas","text":""},{"location":"funcoes_cdd/#configurar_engineschema","title":"<code>configurar_engine(schema)</code>","text":"<p>Configura a conex\u00e3o com o banco de dados Snowflake, utilizando credenciais e informa\u00e7\u00f5es armazenadas em vari\u00e1veis de ambiente. A fun\u00e7\u00e3o retorna um objeto <code>engine</code> para intera\u00e7\u00e3o com o banco de dados.</p>"},{"location":"funcoes_cdd/#carregar_dadosquery-engine","title":"<code>carregar_dados(query, engine)</code>","text":"<p>Executa uma consulta SQL e retorna os resultados em um <code>DataFrame</code> do pandas.</p>"},{"location":"funcoes_cdd/#gerar_hashrow-colunasnone","title":"<code>gerar_hash(row, colunas=None)</code>","text":"<p>Gera um hash SHA-256 com base nas colunas especificadas de uma linha de dados ou de toda a linha se nenhuma coluna for especificada.</p>"},{"location":"funcoes_cdd/#inserir_dados_com_tratamentodf-nome_tabela-engine_destino","title":"<code>inserir_dados_com_tratamento(df, nome_tabela, engine_destino)</code>","text":"<p>Insere dados de um <code>DataFrame</code> em uma tabela de banco de dados PostgreSQL com tratamento de duplicados. A fun\u00e7\u00e3o utiliza a cl\u00e1usula <code>ON CONFLICT</code> para evitar duplica\u00e7\u00e3o de dados com base no campo <code>hash</code>.</p>"},{"location":"funcoes_cdd/#drop_viewsengine_destino","title":"<code>drop_views(engine_destino)</code>","text":"<p>Exclui views espec\u00edficas no banco de dados de destino (PostgreSQL), removendo depend\u00eancias associadas com a cl\u00e1usula <code>CASCADE</code>.</p>"},{"location":"funcoes_cdd/#recreate_viewsengine_destino","title":"<code>recreate_views(engine_destino)</code>","text":"<p>Recria as views espec\u00edficas no banco de dados de destino (PostgreSQL) executando as queries necess\u00e1rias para cada view.</p>"},{"location":"funcoes_cdd/#execucao_no_periodo_permitido","title":"<code>execucao_no_periodo_permitido()</code>","text":"<p>Verifica se o dia atual est\u00e1 dentro de uma janela de execu\u00e7\u00e3o permitida, definida entre os dias 15 e o \u00faltimo dia do m\u00eas.</p>"},{"location":"funcoes_cdd/#funcoes-de-manipulacao-de-data","title":"Fun\u00e7\u00f5es de Manipula\u00e7\u00e3o de Data:","text":"<ul> <li><code>carregar_ultima_data(data_file)</code>: Carrega a \u00faltima data salva de um arquivo.</li> <li><code>salvar_ultima_data(data_file, data)</code>: Salva a data atual em um arquivo.</li> </ul>"},{"location":"funcoes_cdd/#pasta_localsubpasta","title":"<code>pasta_local(subpasta)</code>","text":"<p>Retorna o caminho do diret\u00f3rio local onde os arquivos devem ser salvos, criando a pasta, se necess\u00e1rio.</p>"},{"location":"funcoes_cdd/#3-funcoes-de-processamento-de-arquivos","title":"3. Fun\u00e7\u00f5es de Processamento de Arquivos","text":""},{"location":"funcoes_cdd/#processar_arquivocaminho_destino-aba","title":"<code>processar_arquivo(caminho_destino, aba)</code>","text":"<p>L\u00ea uma aba espec\u00edfica de um arquivo Excel e reescreve a aba no mesmo arquivo, mantendo o formato original. Utiliza a biblioteca <code>pandas</code> e <code>openpyxl</code> para manipula\u00e7\u00e3o de arquivos Excel.</p>"},{"location":"funcoes_cdd/#determinar_modalentidade","title":"<code>determinar_modal(entidade)</code>","text":"<p>Classifica uma entidade em categorias como \"B2B\", \"M. P\u00daBLICO\", \"VAREJO\" ou \"Indefinido\", dependendo do nome da entidade.</p>"},{"location":"funcoes_cdd/#determinar_produtonome","title":"<code>determinar_produto(nome)</code>","text":"<p>Classifica o tipo de produto com base no nome fornecido. As categorias podem incluir \"ISOLADO 10 ML\", \"ISOLADO 30 ML\", \"EXTRATO 36,76\", ou \"Indefinido\".</p>"},{"location":"funcoes_cdd/#4-variaveis-de-configuracao","title":"4. Vari\u00e1veis de Configura\u00e7\u00e3o","text":"<ul> <li><code>DATABASE_URL_DESTINO</code>: A URL de conex\u00e3o com o banco de dados PostgreSQL de destino.</li> <li><code>engine_destino</code>: O objeto <code>engine</code> configurado para conex\u00e3o com o PostgreSQL.</li> </ul>"},{"location":"funcoes_cdd/#5-carregamento-de-variaveis-de-ambiente","title":"5. Carregamento de Vari\u00e1veis de Ambiente","text":"<p>O script utiliza o arquivo <code>.env</code> para carregar vari\u00e1veis sens\u00edveis como credenciais de banco de dados e outras configura\u00e7\u00f5es importantes:</p> <pre><code>ACCOUNT=example_account\nUSER=example_user\nPASSWORD_SNOW=example_password\nDATABASE=example_database\nSCHEMA=example_schema\nWAREHOUSE=example_warehouse\nROLE=example_role\n</code></pre>"},{"location":"introducao_views/","title":"Fonte de Dados","text":""},{"location":"introducao_views/#tabelas-necessarias-diretamente","title":"Tabelas Necess\u00e1rias (Diretamente)","text":""},{"location":"introducao_views/#1-fato-vendas","title":"1. Fato Vendas","text":"<p>Colunas: - Data da Venda - C\u00f3digo do Consultor T\u00e9cnico (<code>cod_ct</code>) - C\u00f3digo do Produto (<code>cod_produto</code>) - Unidades Vendidas  </p>"},{"location":"introducao_views/#2-fato-meta","title":"2. Fato Meta","text":"<p>Colunas: - Data (M\u00eas da Meta) - C\u00f3digo do Consultor T\u00e9cnico (<code>cod_ct</code>) - C\u00f3digo do Produto (<code>cod_produto</code>) - Unidades Meta  </p>"},{"location":"introducao_views/#3-dimensao-ct","title":"3. Dimens\u00e3o CT","text":"<p>Colunas: - C\u00f3digo do Consultor T\u00e9cnico (<code>cod_ct</code>) - Nome do Consultor T\u00e9cnico (<code>nome_ct</code>) - Data de Admiss\u00e3o - Data de Demiss\u00e3o  </p>"},{"location":"introducao_views/#4-dimensao-gr","title":"4. Dimens\u00e3o GR","text":"<p>Colunas: - C\u00f3digo do Gerente Regional (<code>cod_gr</code>) - Nome do Gerente Regional (<code>nome_gr</code>) - Data de Admiss\u00e3o - Data de Demiss\u00e3o  </p>"},{"location":"introducao_views/#5-scd-slow-changing-dimension-nivel-do-ct","title":"5. SCD (Slow Changing Dimension) N\u00edvel do CT","text":"<p>Colunas: - C\u00f3digo do Consultor T\u00e9cnico (<code>cod_ct</code>) - N\u00edvel - Data de In\u00edcio do N\u00edvel (<code>data_inicio_nivel</code>) - Data de Fim do N\u00edvel (<code>data_fim_nivel</code>)  </p>"},{"location":"introducao_views/#6-scd-slow-changing-dimension-nivel-do-gr","title":"6. SCD (Slow Changing Dimension) N\u00edvel do GR","text":"<p>Colunas: - C\u00f3digo do Gerente Regional (<code>cod_gr</code>) - N\u00edvel - Data de In\u00edcio do N\u00edvel (<code>data_inicio_nivel</code>) - Data de Fim do N\u00edvel (<code>data_fim_nivel</code>)  </p>"},{"location":"introducao_views/#7-scd-bonus-ct","title":"7. SCD B\u00f4nus CT","text":"<p>Colunas: - Data (M\u00eas de Vig\u00eancia dos Valores) - N\u00edvel - B\u00f4nus Meta 1 - B\u00f4nus Meta 2 - B\u00f4nus Meta 3  </p>"},{"location":"introducao_views/#8-scd-bonus-gr","title":"8. SCD B\u00f4nus GR","text":"<p>Colunas: - Data (M\u00eas de Vig\u00eancia dos Valores) - N\u00edvel - B\u00f4nus Meta 1 - B\u00f4nus Meta 2 - B\u00f4nus Meta 3  </p>"},{"location":"introducao_views/#tabelas-necessarias-indiretamente","title":"Tabelas Necess\u00e1rias (Indiretamente)","text":""},{"location":"introducao_views/#1-scd-ct-territorio","title":"1. SCD CT Territ\u00f3rio","text":"<p>Colunas: - C\u00f3digo do Consultor T\u00e9cnico (<code>cod_ct</code>) - C\u00f3digo do Territ\u00f3rio (<code>cod_territorio</code>) - C\u00f3digo do Gerente Regional (<code>cod_gr</code>) - Data de In\u00edcio da Atua\u00e7\u00e3o no Territ\u00f3rio (<code>data_inicio_atuacao_territorio</code>) - Data de Fim da Atua\u00e7\u00e3o no Territ\u00f3rio (<code>data_fim_atuacao_territorio</code>)  </p>"},{"location":"introducao_views/#obtencao-da-tabela-de-vendas-consolidada","title":"Obten\u00e7\u00e3o da Tabela de Vendas Consolidada","text":""},{"location":"introducao_views/#objetivo","title":"Objetivo","text":"<p>Consolidar todas as vendas em uma \u00fanica tabela unificada, contendo dados de m\u00faltiplas fontes, com as seguintes colunas principais: - Data da Venda - C\u00f3digo do Consultor T\u00e9cnico (<code>cod_ct</code>) - C\u00f3digo do Produto (<code>cod_produto</code>) - Unidades Vendidas  </p>"},{"location":"introducao_views/#fontes-de-dados","title":"Fontes de Dados","text":"<ul> <li>Vendas CDD  </li> <li>Vendas Extras  </li> <li>Vendas Mercado P\u00fablico e Sa\u00fade Suplementar  </li> <li>Vendas Vouchers (considerando apenas vendas com desconto acima de 99%)  </li> </ul>"},{"location":"introducao_views/#1o-passo-padronizacao-de-cada-tabela-de-vendas","title":"1\u00ba Passo \u2013 Padroniza\u00e7\u00e3o de cada tabela de vendas","text":"<p>Neste passo, foi realizado um processo de padroniza\u00e7\u00e3o em cada uma das tabelas de vendas (CDD, Extras, Acesso e Vouchers) com o objetivo de gerar a coluna cod_territorio. Essa coluna \u00e9 essencial para determinar o CT respons\u00e1vel por cada venda.</p> <p></p>"},{"location":"introducao_views/#observacoes-importantes","title":"Observa\u00e7\u00f5es Importantes:","text":"<ol> <li>Tabela de Vendas CDD </li> <li>Foram exclu\u00eddas as vendas do canal HOSPITALAR.  </li> <li> <p>Tamb\u00e9m foram removidas as vendas cujos informantes pertencem \u00e0s categorias DIMED, LS e PORTAL.  </p> </li> <li> <p>Tabela de Vendas VOUCHER </p> </li> <li>Foram mantidas apenas as vendas que apresentam desconto superior a 70%.  </li> <li>Para vouchers de 99.99%, descontamos 1 voucher por uma venda.  </li> <li>Para vouchers entre 70% e 80%, descontamos 2 vendas a cada 3 vouchers, ou seja, 1 voucher de 70% equivale a 2/3 de uma venda.  </li> </ol>"},{"location":"introducao_views/#resultado-final","title":"Resultado Final","text":"<p>Ao final deste passo, cada tabela foi ajustada para conter apenas os dados relevantes, resultando na seguinte estrutura padr\u00e3o:</p> <p></p>"},{"location":"introducao_views/#2o-passo-atribuicao-do-codigo-ct-as-vendas","title":"2\u00ba Passo: Atribui\u00e7\u00e3o do C\u00d3DIGO CT \u00e0s Vendas","text":"<p>No passo anterior, foram geradas quatro tabelas (CDD, Extras, Acesso e Vouchers) no formato da <code>fato_vendas_pre_consolidada</code>.  </p> <p>Neste passo, o objetivo \u00e9 atribuir a coluna <code>cod_ct</code>, garantindo que as vendas sejam computadas corretamente para o Consultor T\u00e9cnico (CT) respons\u00e1vel, mesmo que o <code>cod_territorio</code> tenha mudado de CT ao longo do tempo. Essa abordagem \u00e9 necess\u00e1ria para refletir as mudan\u00e7as hist\u00f3ricas de aloca\u00e7\u00e3o dos territ\u00f3rios.  </p> <p>Para atingir esse objetivo, utilizamos um LEFT JOIN entre a tabela <code>fato_vendas_pre_consolidada</code> e a dimens\u00e3o <code>dim_ct_territorio</code>. Essa liga\u00e7\u00e3o \u00e9 feita considerando tanto o <code>cod_territorio</code> quanto o intervalo de datas:  </p> <ul> <li>O <code>cod_territorio</code> da tabela fato \u00e9 comparado com o da dimens\u00e3o.  </li> <li>A data da venda \u00e9 verificada para estar entre <code>data_inicio_atuacao</code> e <code>data_fim_atuacao</code> (ou a data atual, caso a sa\u00edda n\u00e3o esteja definida).  </li> </ul> <p></p> <p></p>"},{"location":"introducao_views/#resultado-final_1","title":"Resultado Final","text":"<p>Ao final deste passo, cada tabela foi ajustada para conter apenas os dados relevantes, resultando na seguinte estrutura padr\u00e3o:</p> <p></p>"},{"location":"introducao_views/#3o-passo-agrupamento-das-vendas-por-data-e-concatenacao-das-tabelas","title":"3\u00ba Passo: Agrupamento das vendas por data e Concatena\u00e7\u00e3o das tabelas","text":"<p>Por fim, cada tabela fato foi agrupada por data, cod_ct e cod_produto, obtendo a soma das vendas. Em seguida, foram criadas as colunas N\u00edvel e Tempo de Empresa, referentes ao n\u00edvel de tempo de empresa do CT no per\u00edodo filtrado.</p> <p></p>"},{"location":"introducao_views/#por-que-foi-necessario-esse-ultimo-passo","title":"Por que foi necess\u00e1rio esse \u00faltimo passo?","text":"<p>Foi uma solu\u00e7\u00e3o que eu encontrei para conseguir estabelecer um link entre o CT e a dimens\u00e3o calend\u00e1rio.</p>"},{"location":"sharepoint_downloader/","title":"SharePoint Downloader","text":"<p>Este script \u00e9 uma implementa\u00e7\u00e3o de uma classe em Python para interagir com o SharePoint, permitindo a autentica\u00e7\u00e3o, obten\u00e7\u00e3o de arquivos e download de arquivos de uma pasta remota no SharePoint para um diret\u00f3rio local. Ele usa a biblioteca <code>office365-rest-python-client</code> para interagir com o SharePoint e a biblioteca <code>dotenv</code> para carregar as credenciais de autentica\u00e7\u00e3o de um arquivo <code>.env</code>.</p>"},{"location":"sharepoint_downloader/#estrutura-do-codigo","title":"Estrutura do C\u00f3digo","text":""},{"location":"sharepoint_downloader/#bibliotecas-importadas","title":"Bibliotecas Importadas","text":"<ul> <li>os: Usada para interagir com o sistema de arquivos.</li> <li>office365.sharepoint.client_context: Importa o contexto de cliente para interagir com o SharePoint.</li> <li>office365.sharepoint.files.file: Usada para trabalhar com arquivos no SharePoint.</li> <li>office365.runtime.auth.authentication_context: Usada para autenticar o usu\u00e1rio no SharePoint.</li> <li>dotenv: Biblioteca para carregar vari\u00e1veis de ambiente a partir de um arquivo <code>.env</code>.</li> </ul>"},{"location":"sharepoint_downloader/#carregamento-de-credenciais","title":"Carregamento de Credenciais","text":"<p>O arquivo <code>.env</code> \u00e9 carregado utilizando a fun\u00e7\u00e3o <code>load_dotenv()</code> para acessar as vari\u00e1veis de ambiente que cont\u00eam o nome de usu\u00e1rio (<code>SHARE_USER</code>) e a senha (<code>SHARE_PASS</code>) para autentica\u00e7\u00e3o no SharePoint.</p>"},{"location":"sharepoint_downloader/#urls-e-pastas-remotas","title":"URLs e Pastas Remotas","text":"<ul> <li><code>site_url</code>: Cont\u00e9m dois links para diferentes sites SharePoint.</li> <li><code>pasta_remota</code>: Cont\u00e9m caminhos relativos para v\u00e1rias pastas no SharePoint onde os arquivos desejados podem ser encontrados.</li> </ul>"},{"location":"sharepoint_downloader/#classe-sharepointdownloader","title":"Classe <code>SharePointDownloader</code>","text":""},{"location":"sharepoint_downloader/#atributos","title":"Atributos","text":"<ul> <li>site_url: URL do site SharePoint ao qual voc\u00ea deseja se conectar.</li> <li>username e password: Credenciais para autentica\u00e7\u00e3o no SharePoint.</li> <li>ctx: Um objeto <code>ClientContext</code> autenticado que \u00e9 usado para realizar as opera\u00e7\u00f5es de intera\u00e7\u00e3o com o SharePoint.</li> </ul>"},{"location":"sharepoint_downloader/#metodos","title":"M\u00e9todos","text":"<ul> <li> <p><code>__init__(self, site_url, username, password)</code>:   Inicializa a classe, define os valores para <code>site_url</code>, <code>username</code>, e <code>password</code>, e chama o m\u00e9todo <code>_autenticar()</code> para obter o contexto de conex\u00e3o autenticado.</p> </li> <li> <p><code>_autenticar(self)</code>:   Realiza a autentica\u00e7\u00e3o no SharePoint utilizando o nome de usu\u00e1rio e senha fornecidos. Caso a autentica\u00e7\u00e3o seja bem-sucedida, retorna o contexto <code>ClientContext</code> para uso posterior.</p> </li> <li> <p><code>obter_arquivos_da_pasta(self, pasta_remota)</code>:   Obt\u00e9m os arquivos de uma pasta espec\u00edfica no SharePoint. Retorna uma lista de arquivos encontrados na pasta.</p> </li> <li> <p><code>baixar_arquivo(self, arquivo_remoto, caminho_destino)</code>:   Baixa um arquivo individual do SharePoint para um caminho de destino local especificado.</p> </li> <li> <p><code>baixar_todos_arquivos(self, pasta_remota, pasta_local)</code>:   Baixa todos os arquivos de uma pasta remota do SharePoint para um diret\u00f3rio local.</p> </li> </ul>"},{"location":"sharepoint_downloader/#como-funciona-o-codigo","title":"Como Funciona o C\u00f3digo","text":"<ol> <li> <p>Autentica\u00e7\u00e3o: Quando a classe <code>SharePointDownloader</code> \u00e9 instanciada, ela tenta autenticar o usu\u00e1rio no SharePoint usando as credenciais fornecidas. Se a autentica\u00e7\u00e3o for bem-sucedida, o contexto de cliente \u00e9 criado.</p> </li> <li> <p>Obter Arquivos: Para obter arquivos de uma pasta, o m\u00e9todo <code>obter_arquivos_da_pasta()</code> \u00e9 chamado com o caminho relativo da pasta no SharePoint. O SharePoint retorna uma lista de arquivos na pasta, que \u00e9 ent\u00e3o utilizada para fazer o download dos arquivos.</p> </li> <li> <p>Baixar Arquivos: O m\u00e9todo <code>baixar_arquivo()</code> baixa um arquivo espec\u00edfico do SharePoint para o diret\u00f3rio local. O caminho do arquivo remoto no SharePoint e o caminho local onde o arquivo ser\u00e1 salvo s\u00e3o fornecidos como par\u00e2metros.</p> </li> <li> <p>Baixar Todos os Arquivos: O m\u00e9todo <code>baixar_todos_arquivos()</code> \u00e9 utilizado para baixar todos os arquivos de uma pasta. Ele chama <code>obter_arquivos_da_pasta()</code> para listar os arquivos e, em seguida, usa <code>baixar_arquivo()</code> para baixar cada arquivo individualmente.</p> </li> </ol>"},{"location":"sharepoint_downloader/#conclusao","title":"Conclus\u00e3o","text":"<p>Essa implementa\u00e7\u00e3o pode ser utilizada em v\u00e1rios cen\u00e1rios para automatizar o processo de obten\u00e7\u00e3o e download de arquivos de um SharePoint. Ele oferece uma maneira simples de conectar-se ao SharePoint, autenticar-se e fazer o download de arquivos para o sistema local.</p>"},{"location":"update_dim_cddd/","title":"Atualiza\u00e7\u00e3o de Dimens\u00f5es CDDD","text":"<p>Este script realiza o processo de ETL (Extra\u00e7\u00e3o, Transforma\u00e7\u00e3o e Carregamento) para atualizar as tabelas de dimens\u00f5es (DIM) da schema CDD. O processo envolve a extra\u00e7\u00e3o de dados das tabelas especificadas, aplica\u00e7\u00e3o de transforma\u00e7\u00f5es necess\u00e1rias e inser\u00e7\u00e3o dos dados tratados no banco de dados de destino.</p>"},{"location":"update_dim_cddd/#tabelas-processadas","title":"Tabelas Processadas","text":"<p>O script processa as seguintes tabelas dentro da schema <code>CDD</code>:</p> <ul> <li> <p>FORCA_VENDAS</p> </li> <li> <p>PDVS</p> </li> <li> <p>CANAL</p> </li> <li> <p>INFORMANTES</p> </li> <li> <p>APRES</p> </li> <li> <p>FAB</p> </li> <li> <p>FV_DISTRITO</p> </li> <li> <p>FV_REGIONAL</p> </li> <li> <p>FV_TERRITORIO</p> </li> <li> <p>PROD</p> </li> <li> <p>TIPO_TX</p> </li> <li> <p>UTC</p> </li> </ul>"},{"location":"update_dim_cddd/#funcoes-implementadas","title":"Fun\u00e7\u00f5es Implementadas","text":""},{"location":"update_dim_cddd/#1-atualizar_dim_cddd","title":"1. <code>atualizar_dim_cddd()</code>","text":"<p>Fun\u00e7\u00e3o principal que executa o processo de ETL:</p> <ul> <li>Extra\u00e7\u00e3o: Carrega os dados das tabelas especificadas.</li> <li> <p>Transforma\u00e7\u00e3o:</p> </li> <li> <p>Para a tabela <code>PDVS</code>, substitui valores nulos por uma data padr\u00e3o e converte a coluna <code>data_abertura</code> para o formato <code>datetime</code>.</p> </li> <li> <p>Carregamento: Insere os dados tratados no banco de dados de destino.</p> </li> </ul>"},{"location":"update_dim_cddd/#2-gerar_hash","title":"2. <code>gerar_hash()</code>","text":"<p>Cria um hash \u00fanico para cada linha de dados, utilizando as colunas especificadas.</p>"},{"location":"update_dim_cddd/#3-inserir_dados_com_tratamento","title":"3. <code>inserir_dados_com_tratamento()</code>","text":"<p>Insere os dados tratados nas tabelas do banco de dados de destino.</p>"},{"location":"update_dim_cddd/#detalhamento-das-transformacoes","title":"Detalhamento das Transforma\u00e7\u00f5es","text":"<ul> <li>PDVS: A coluna <code>data_abertura</code> \u00e9 preenchida com a data padr\u00e3o <code>1900-01-01</code> nos casos de valores nulos, convertida para o formato <code>datetime</code> e as colunas <code>decada_abertura</code>, <code>data_consulta</code>, e <code>data_situacao</code> s\u00e3o removidas.</li> <li>Hashing: Para tabelas como <code>FORCA_VENDAS</code>, <code>PDVS</code>, <code>INFORMANTES</code>, <code>FV_DISTRITO</code>, <code>FV_REGIONAL</code>, <code>FV_TERRITORIO</code> e <code>UTC</code>, \u00e9 gerado um hash \u00fanico utilizando colunas espec\u00edficas.</li> </ul>"},{"location":"update_dim_cddd/#execucao","title":"Execu\u00e7\u00e3o","text":"<p>A execu\u00e7\u00e3o do processo \u00e9 realizada ao rodar o script principal <code>atualizar_dim_cddd()</code>. Isso ir\u00e1:</p> <ol> <li>Processar os dados de todas as tabelas.</li> <li>Aplicar as transforma\u00e7\u00f5es necess\u00e1rias.</li> <li>Inserir os dados tratados no banco de dados de destino.</li> </ol>"},{"location":"update_dim_cddd/#dependencias","title":"Depend\u00eancias","text":"<p>O script depende das seguintes bibliotecas: - <code>pandas</code> - <code>sqlalchemy</code> - <code>snowflake.sqlalchemy</code> - <code>datetime</code></p> <p>Al\u00e9m disso, o script tamb\u00e9m utiliza fun\u00e7\u00f5es auxiliares importadas do m\u00f3dulo <code>projeto_banco_cdd.modulos.functions_cdd</code>.</p>"},{"location":"update_dim_cddd/#observacoes","title":"Observa\u00e7\u00f5es","text":"<pre><code>- O script foi projetado para funcionar com a schema `CDD`, mas pode ser modificado para outros schemas se necess\u00e1rio.\n- As transforma\u00e7\u00f5es podem ser ajustadas conforme a necessidade das tabelas processadas.\n- A fun\u00e7\u00e3o `atualizar_dim_cddd()` \u00e9 a principal fun\u00e7\u00e3o de execu\u00e7\u00e3o, chamada automaticamente ao rodar o script.\n</code></pre>"},{"location":"update_extras_acesso_scd/","title":"Atualiza\u00e7\u00e3o de Extras, Acesso e Slow Changing Dimensions (SCD)","text":"<p>Este script realiza o processo de atualiza\u00e7\u00e3o dos dados de Extras, Acesso e Slow Changing Dimensions (SCD) no banco de dados a partir de arquivos armazenados no SharePoint. O processo envolve a extra\u00e7\u00e3o, processamento e armazenamento das informa\u00e7\u00f5es nas tabelas do banco de dados de destino.</p>"},{"location":"update_extras_acesso_scd/#funcoes-implementadas","title":"Fun\u00e7\u00f5es Implementadas","text":""},{"location":"update_extras_acesso_scd/#1-atualizar_extras_acesso_scd","title":"1. <code>atualizar_extras_acesso_scd()</code>","text":"<p>Fun\u00e7\u00e3o principal que executa o processo de atualiza\u00e7\u00e3o dos dados de Extras, Acesso e SCD: - Baixar Arquivos: A fun\u00e7\u00e3o baixa os arquivos Excel dos sites do SharePoint utilizando a classe <code>SharePointDownloader</code>. - Processamento de Arquivos: Ap\u00f3s o download, os arquivo s\u00e3o processados, limpando os dados e ajustando os tipos de colunas, como a convers\u00e3o de datas. - Armazenamento no Banco: Ap\u00f3s o processamento, os dados s\u00e3o armazenados no banco de dados de destino utilizando a fun\u00e7\u00e3o <code>to_sql</code>.</p>"},{"location":"update_extras_acesso_scd/#subprocessos","title":"Subprocessos","text":"<ol> <li>Baixar e Processar Arquivos: Arquivos de Extras, Acesso e SCD s\u00e3o baixados do SharePoint e processados.</li> <li>Limpeza de Dados: A data de <code>Acesso</code> \u00e9 convertida para o tipo <code>datetime</code> e registros com dados inv\u00e1lidos s\u00e3o removidos.</li> <li>Inser\u00e7\u00e3o de Dados:</li> <li>Dados de Extras e Acesso s\u00e3o inseridos nas tabelas <code>fato_extras</code> e <code>fato_acesso</code>, respectivamente.</li> <li>As tabelas de dimens\u00f5es SCD s\u00e3o inseridas no banco com os dados processados.</li> </ol>"},{"location":"update_extras_acesso_scd/#2-baixar_e_processar","title":"2. <code>baixar_e_processar()</code>","text":"<p>Fun\u00e7\u00e3o auxiliar para baixar e processar arquivos de maneira eficiente: - Baixa o arquivo do SharePoint. - Processa o arquivo Excel, convertendo os dados conforme necess\u00e1rio.</p>"},{"location":"update_extras_acesso_scd/#detalhamento-das-transformacoes","title":"Detalhamento das Transforma\u00e7\u00f5es","text":"<ul> <li>Extras: Os dados de extras s\u00e3o baixados e inseridos na tabela <code>fato_extras</code>.</li> <li>Acesso: A coluna <code>Data</code> da tabela de Acesso \u00e9 convertida para o formato <code>datetime</code> e registros inv\u00e1lidos (com data inv\u00e1lida) s\u00e3o removidos.</li> <li>SCD: As dimens\u00f5es do SCD s\u00e3o processadas, com a data de <code>data_fim_gerenciamento</code> sendo convertida para o tipo <code>datetime</code> quando presente. As dimens\u00f5es s\u00e3o inseridas em tabelas correspondentes no banco de dados.</li> </ul>"},{"location":"update_extras_acesso_scd/#dependencias","title":"Depend\u00eancias","text":"<p>O script depende das seguintes bibliotecas: - <code>os</code> - <code>pandas</code></p> <p>Al\u00e9m disso, o script utiliza fun\u00e7\u00f5es auxiliares do m\u00f3dulo <code>projeto_banco_cdd.modulos.sharepoint_downloader</code> para baixar os arquivos do SharePoint e do m\u00f3dulo <code>projeto_banco_cdd.modulos.functions_cdd</code> para processar e armazenar os dados.</p>"},{"location":"update_extras_acesso_scd/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>Os arquivos de dados de Extras, Acesso e SCD s\u00e3o baixados de diferentes sites do SharePoint, conforme configurado no dicion\u00e1rio <code>site_url</code>.</li> <li>O banco de dados de destino \u00e9 especificado atrav\u00e9s da vari\u00e1vel <code>engine_destino</code>.</li> <li>As tabelas de dados s\u00e3o substitu\u00eddas nas tabelas do banco de dados de destino, utilizando a op\u00e7\u00e3o <code>if_exists='replace'</code> para sobrescrever os dados existentes.</li> </ul>"},{"location":"update_extras_acesso_scd/#exemplos-de-uso","title":"Exemplos de Uso","text":"<p>```python</p>"},{"location":"update_extras_acesso_scd/#rodando-o-script","title":"Rodando o script","text":"<p>if name == \"main\":     atualizar_extras_acesso_scd()</p>"},{"location":"update_fato_cdd/","title":"Atualiza\u00e7\u00e3o da Tabela Fato CDDD","text":""},{"location":"update_fato_cdd/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este documento detalha o processo de ETL (Extra\u00e7\u00e3o, Transforma\u00e7\u00e3o e Carga) aplicado ao schema <code>CDDD</code> e sua tabela fato. O processo abrange a extra\u00e7\u00e3o de dados, a aplica\u00e7\u00e3o de transforma\u00e7\u00f5es e a inser\u00e7\u00e3o dos dados processados no banco de dados de destino.</p>"},{"location":"update_fato_cdd/#objetivo","title":"Objetivo","text":"<p>O objetivo \u00e9 manter a integridade dos dados e garantir que a tabela fato seja atualizada no banco de dados de destino diariamente, aplicando as transforma\u00e7\u00f5es necess\u00e1rias para a consist\u00eancia e a formata\u00e7\u00e3o correta.</p>"},{"location":"update_fato_cdd/#arquivos-e-dependencias","title":"Arquivos e Depend\u00eancias","text":"<p>O processo depende de alguns arquivos de configura\u00e7\u00e3o e m\u00f3dulos auxiliares:</p> <ul> <li> <p>M\u00f3dulos de fun\u00e7\u00f5es:</p> <ul> <li> <p><code>configurar_engine</code>: Configura\u00e7\u00e3o das conex\u00f5es com o banco de dados.</p> </li> <li> <p><code>carregar_dados</code>: Fun\u00e7\u00e3o para carregar os dados das tabelas.</p> </li> <li> <p><code>gerar_hash</code>: Fun\u00e7\u00e3o para gerar um hash \u00fanico para cada linha de dados.</p> </li> <li> <p><code>inserir_dados_com_tratamento</code>: Fun\u00e7\u00e3o para inserir os dados tratados no banco de dados de destino.</p> </li> </ul> </li> </ul>"},{"location":"update_fato_cdd/#estrutura-de-dados","title":"Estrutura de Dados","text":"<p>A tabela envolvida neste processo \u00e9:</p> <ul> <li>Tabela <code>FATO</code> no esquema <code>cddd</code>: Cont\u00e9m dados principais de unidades e identificadores de registros.</li> </ul>"},{"location":"update_fato_cdd/#processo-de-etl","title":"Processo de ETL","text":""},{"location":"update_fato_cdd/#1-carregar-dados","title":"1. Carregar Dados","text":"<p>A fun\u00e7\u00e3o come\u00e7a carregando os dados do schemas <code>cddd</code>. </p>"},{"location":"update_fato_cdd/#2-aplicar-transformacoes","title":"2. Aplicar Transforma\u00e7\u00f5es","text":"<p>As transforma\u00e7\u00f5es aplicadas nos dados incluem:</p> <ul> <li> <p>Convers\u00e3o de colunas de data para o formato <code>YYYY-MM</code>.</p> </li> <li> <p>Renomea\u00e7\u00e3o de colunas financeiras (de <code>r$</code> para <code>valor_</code>). Essa renomea\u00e7\u00e3o \u00e9 feita porque o Postgresql n\u00e3o aceita o $ como nome de coluna.</p> </li> <li> <p>Renomea\u00e7\u00e3o da coluna <code>cod_anomesdia</code> para <code>cod_anomes</code>. Essa altera\u00e7\u00e3o \u00e9 necess\u00e1ria porque a tabela j\u00e1 carregada no banco de dados se origina do CDD-Mensal, que utiliza essa nomenclatura. A padroniza\u00e7\u00e3o evita inconsist\u00eancias na inser\u00e7\u00e3o de dados devido a diferen\u00e7as nos nomes das colunas.</p> </li> <li> <p>Convers\u00e3o de valores financeiros e num\u00e9ricos para o formato adequado.</p> </li> </ul>"},{"location":"update_fato_cdd/#3-gerar-hash","title":"3. Gerar Hash","text":"<p>Ap\u00f3s as transforma\u00e7\u00f5es, um hash \u00fanico \u00e9 gerado para cada linha de dados para garantir a unicidade e integridade dos registros.</p>"},{"location":"update_fato_cdd/#4-inserir-dados-no-banco-de-dados","title":"4 . Inserir Dados no Banco de Dados","text":"<p>Por fim, os dados tratados s\u00e3o inseridos no banco de dados de destino, utilizando a fun\u00e7\u00e3o <code>inserir_dados_com_tratamento</code>.</p>"},{"location":"update_fato_cdd/#conclusao","title":"Conclus\u00e3o","text":"<p>O processo de ETL descrito neste documento garante a atualiza\u00e7\u00e3o e integridade dos dados nas tabela <code>FATO</code> do schema <code>cddd</code>. Ele realiza transforma\u00e7\u00f5es nos dados, gera hashes para garantir a unicidade dos registros e insere os dados no banco de dados de destino.</p> <p>Esse fluxo garante que a tabela de fato seja mantida atualizada e que as transforma\u00e7\u00f5es necess\u00e1rias sejam aplicadas de forma consistente.</p>"},{"location":"update_fato_td/","title":"Atualiza\u00e7\u00e3o da Tabela Fato TD","text":""},{"location":"update_fato_td/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este documento detalha o processo de ETL (Extra\u00e7\u00e3o, Transforma\u00e7\u00e3o e Carga) aplicado ao schema <code>TD</code> e sua tabela fato. O processo abrange a extra\u00e7\u00e3o de dados, a aplica\u00e7\u00e3o de transforma\u00e7\u00f5es e a inser\u00e7\u00e3o dos dados processados no banco de dados de destino.</p>"},{"location":"update_fato_td/#objetivo","title":"Objetivo","text":"<p>O objetivo \u00e9 manter a integridade dos dados e garantir que a tabela fato seja atualizada no banco de dados de destino diariamente, aplicando as transforma\u00e7\u00f5es necess\u00e1rias para a consist\u00eancia e a formata\u00e7\u00e3o correta.</p>"},{"location":"update_fato_td/#arquivos-e-dependencias","title":"Arquivos e Depend\u00eancias","text":"<p>O processo depende de alguns arquivos de configura\u00e7\u00e3o e m\u00f3dulos auxiliares:</p> <ul> <li> <p><code>ultima_data_fato_td.txt</code>: Cont\u00e9m a \u00faltima data de fato TD processada.</p> </li> <li> <p>M\u00f3dulos de fun\u00e7\u00f5es:</p> <ul> <li> <p><code>configurar_engine</code>: Configura\u00e7\u00e3o das conex\u00f5es com o banco de dados.</p> </li> <li> <p><code>carregar_dados</code>: Fun\u00e7\u00e3o para carregar os dados das tabelas.</p> </li> <li> <p><code>gerar_hash</code>: Fun\u00e7\u00e3o para gerar um hash \u00fanico para cada linha de dados.</p> </li> <li> <p><code>inserir_dados_com_tratamento</code>: Fun\u00e7\u00e3o para inserir os dados tratados no banco de dados de destino.</p> </li> <li> <p><code>execucao_no_periodo_permitido</code>: Fun\u00e7\u00e3o para verificar se o script est\u00e1 dentro do per\u00edodo de execu\u00e7\u00e3o.</p> </li> </ul> </li> </ul>"},{"location":"update_fato_td/#parametros","title":"Par\u00e2metros:","text":"<ul> <li><code>DATA_FILE</code>: Caminho do arquivo que cont\u00e9m a \u00faltima data processada. O padr\u00e3o \u00e9 'ultima_data_fato_td.txt'.</li> </ul>"},{"location":"update_fato_td/#processo-de-etl","title":"Processo de ETL","text":""},{"location":"update_fato_td/#verificacao-de-periodo","title":"Verifica\u00e7\u00e3o de per\u00edodo","text":"<p>A fun\u00e7\u00e3o verifica se a data atual est\u00e1 entre os dias 15 e o \u00faltimo dia do m\u00eas. Caso a condi\u00e7\u00e3o seja atendida, o script prossegue com a execu\u00e7\u00e3o; caso contr\u00e1rio, ele \u00e9 interrompido.  </p> <p>Obs: Essa l\u00f3gica evita consultas desnecess\u00e1rias ao banco de dados Snowflake, uma vez que a tabela TD \u00e9 atualizada apenas uma vez por m\u00eas, entre os dias 15 e 30/31. Esse intervalo pode ser reduzido se tivermos uma data exata de atualiza\u00e7\u00e3o da tabela, otimizando ainda mais o n\u00famero de consultas realizadas.</p>"},{"location":"update_fato_td/#1-carregar-dados","title":"1. Carregar Dados","text":"<p>A fun\u00e7\u00e3o come\u00e7a carregando os dados do schema <code>td</code>. A fun\u00e7\u00e3o carrega apenas os dados mais recentes, com base na \u00faltima data salva no arquivo.</p>"},{"location":"update_fato_td/#2-aplicar-transformacoes","title":"2. Aplicar Transforma\u00e7\u00f5es","text":"<p>As transforma\u00e7\u00f5es aplicadas nos dados incluem: - Convers\u00e3o de colunas de data para o formato <code>YYYY-MM</code>. - Renomea\u00e7\u00e3o de colunas financeiras (de <code>r$</code> para <code>valor_</code>). - Convers\u00e3o de valores financeiros e num\u00e9ricos para o formato adequado.</p>"},{"location":"update_fato_td/#3-gerar-hash","title":"3. Gerar Hash","text":"<p>Ap\u00f3s as transforma\u00e7\u00f5es, um hash \u00fanico \u00e9 gerado para cada linha de dados para garantir a unicidade e integridade dos registros.</p>"},{"location":"update_fato_td/#4-atualizar-ultima-data","title":"4. Atualizar \u00daltima Data","text":"<p>A \u00faltima data processada \u00e9 salva no arquivo <code>ultima_data_fato_td.txt</code>, garantindo que, na pr\u00f3xima execu\u00e7\u00e3o, apenas novos dados sejam carregados.</p>"},{"location":"update_fato_td/#5-inserir-dados-no-banco-de-dados","title":"5. Inserir Dados no Banco de Dados","text":"<p>Por fim, os dados tratados s\u00e3o inseridos no banco de dados de destino, utilizando a fun\u00e7\u00e3o <code>inserir_dados_com_tratamento</code>.</p>"},{"location":"update_fato_td/#conclusao","title":"Conclus\u00e3o","text":"<p>O processo de ETL descrito neste documento garante a atualiza\u00e7\u00e3o e integridade dos dados nas tabela <code>FATO</code> do schema <code>td</code>. Ele realiza transforma\u00e7\u00f5es nos dados, gera hashes para garantir a unicidade dos registros e insere os dados no banco de dados de destino.</p> <p>Esse fluxo garante que a tabela de fato seja mantida atualizada e que as transforma\u00e7\u00f5es necess\u00e1rias sejam aplicadas de forma consistente.</p>"},{"location":"update_pbm/","title":"Atualiza\u00e7\u00e3o do PBM","text":"<p>Este script realiza o processo de atualiza\u00e7\u00e3o dos dados do PBM (Product-Based Marketing) a partir de arquivos armazenados no SharePoint. O processo envolve a extra\u00e7\u00e3o, transforma\u00e7\u00e3o e armazenamento das informa\u00e7\u00f5es nas tabelas do banco de dados de destino.</p>"},{"location":"update_pbm/#funcoes-implementadas","title":"Fun\u00e7\u00f5es Implementadas","text":""},{"location":"update_pbm/#1-atualizar_pbm","title":"1. <code>atualizar_pbm()</code>","text":"<p>Fun\u00e7\u00e3o principal que executa o processo de atualiza\u00e7\u00e3o dos dados PBM: - Baixar Arquivos: A fun\u00e7\u00e3o baixa todos os arquivos Excel da pasta correspondente no SharePoint utilizando a classe <code>SharePointDownloader</code>. - Processamento de Arquivos: Ap\u00f3s o download, os arquivos s\u00e3o processados para extrair os dados da folha de nome \"TRANSA\u00c7\u00c3O\". - Armazenamento no Banco: Ap\u00f3s o processamento, os dados s\u00e3o armazenados na tabela <code>fato_pbm</code> no banco de dados de destino.</p>"},{"location":"update_pbm/#subprocessos","title":"Subprocessos","text":"<ol> <li>Baixar Arquivos: A fun\u00e7\u00e3o baixa todos os arquivos Excel armazenados no SharePoint para o diret\u00f3rio local configurado.</li> <li>Processamento de Dados:</li> <li>O script l\u00ea os arquivos Excel, verifica se a aba \"TRANSA\u00c7\u00c3O\" existe e carrega os dados da mesma.</li> <li>Os dados da coluna <code>PRODUTO</code> s\u00e3o corrigidos, substituindo o nome de um produto espec\u00edfico.</li> <li>As colunas <code>CNPJ PDV</code> e <code>EAN</code> t\u00eam os valores de string processados, removendo poss\u00edveis ap\u00f3strofos \u00e0 esquerda.</li> <li>Armazenamento no Banco de Dados: </li> <li>Os dados processados s\u00e3o concatenados em um \u00fanico DataFrame e inseridos na tabela <code>fato_pbm</code> do banco de dados de destino.</li> <li>A fun\u00e7\u00e3o usa o comando <code>to_sql()</code> para inserir os dados, substituindo qualquer dado anterior com <code>if_exists='replace'</code>.</li> </ol>"},{"location":"update_pbm/#2-baixar-arquivos-do-sharepoint","title":"2. Baixar Arquivos do SharePoint","text":"<p>A fun\u00e7\u00e3o <code>baixar_todos_arquivos()</code> da classe <code>SharePointDownloader</code> \u00e9 utilizada para baixar todos os arquivos da pasta remota especificada.</p>"},{"location":"update_pbm/#3-processamento-de-arquivos-excel","title":"3. Processamento de Arquivos Excel","text":"<p>Os arquivos Excel s\u00e3o carregados utilizando a biblioteca <code>pandas</code>. A fun\u00e7\u00e3o verifica se a folha \"TRANSA\u00c7\u00c3O\" existe nos arquivos e, em caso afirmativo, carrega esses dados. Caso contr\u00e1rio, o arquivo \u00e9 ignorado.</p>"},{"location":"update_pbm/#transformacoes-realizadas-nos-dados","title":"Transforma\u00e7\u00f5es Realizadas nos Dados","text":"<ul> <li>Produto: A coluna <code>PRODUTO</code> \u00e9 corrigida para garantir que os nomes dos produtos sigam o formato correto (substitui\u00e7\u00e3o de valores espec\u00edficos).</li> <li>CNPJ PDV e EAN: A fun\u00e7\u00e3o remove caracteres indesejados (ap\u00f3strofos) das colunas <code>CNPJ PDV</code> e <code>EAN</code>.</li> </ul>"},{"location":"update_pbm/#dependencias","title":"Depend\u00eancias","text":"<p>O script depende das seguintes bibliotecas:</p> <ul> <li> <p><code>os</code></p> </li> <li> <p><code>glob</code></p> </li> <li> <p><code>pandas</code></p> </li> <li> <p><code>projeto_banco_cdd.modulos.sharepoint_downloader</code></p> </li> <li> <p><code>projeto_banco_cdd.modulos.functions_cdd</code></p> </li> </ul>"},{"location":"update_pbm/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>O script busca por todos os arquivos Excel na pasta local de destino configurada e processa apenas os que cont\u00eam uma folha de nome \"TRANSA\u00c7\u00c3O\".</li> <li>O banco de dados de destino \u00e9 especificado atrav\u00e9s da vari\u00e1vel <code>engine_destino</code>.</li> <li>O processo de substitui\u00e7\u00e3o na tabela do banco de dados usa a op\u00e7\u00e3o <code>if_exists='replace'</code>, o que garante que os dados existentes sejam substitu\u00eddos.</li> </ul>"},{"location":"update_ticket_medio/","title":"Atualiza\u00e7\u00e3o de Dados de Ticket M\u00e9dio","text":"<p>Este script realiza o processo de atualiza\u00e7\u00e3o dos dados de \"Ticket M\u00e9dio\" a partir de arquivos armazenados no SharePoint. O processo envolve a extra\u00e7\u00e3o, transforma\u00e7\u00e3o e armazenamento das informa\u00e7\u00f5es em uma tabela do banco de dados de destino.</p>"},{"location":"update_ticket_medio/#funcoes-implementadas","title":"Fun\u00e7\u00f5es Implementadas","text":""},{"location":"update_ticket_medio/#1-clean_column_namename","title":"1. <code>clean_column_name(name)</code>","text":"<p>Fun\u00e7\u00e3o que limpa os nomes das colunas, removendo acentos, convertendo para min\u00fasculas e substituindo espa\u00e7os e pontos por underscores.</p>"},{"location":"update_ticket_medio/#parametros","title":"Par\u00e2metros","text":"<ul> <li><code>name</code>: Nome da coluna a ser limpo.</li> </ul>"},{"location":"update_ticket_medio/#retorno","title":"Retorno","text":"<ul> <li>Retorna o nome da coluna limpo.</li> </ul>"},{"location":"update_ticket_medio/#2-atualizar_ticket_medio","title":"2. <code>atualizar_ticket_medio()</code>","text":"<p>Fun\u00e7\u00e3o principal que executa o processo de atualiza\u00e7\u00e3o dos dados de Ticket M\u00e9dio:</p> <ul> <li> <p>Baixar Arquivos: Baixa todos os arquivos Excel armazenados no SharePoint para o diret\u00f3rio local configurado.</p> </li> <li> <p>Processamento de Arquivos: Ap\u00f3s o download, os arquivos s\u00e3o processados para extrair e limpar os dados de Ticket M\u00e9dio.</p> </li> <li> <p>Transforma\u00e7\u00f5es nos Dados: A fun\u00e7\u00e3o realiza v\u00e1rias transforma\u00e7\u00f5es e limpezas nos dados.</p> </li> <li> <p>Armazenamento no Banco: Ap\u00f3s o processamento, os dados s\u00e3o armazenados na tabela <code>fato_ticket_medio</code> no banco de dados de destino.</p> </li> </ul>"},{"location":"update_ticket_medio/#subprocessos","title":"Subprocessos","text":"<ol> <li>Baixar Arquivos: O script baixa todos os arquivos Excel da pasta remota configurada no SharePoint para um diret\u00f3rio local.</li> <li> <p>Processamento de Dados:</p> <ul> <li> <p>Os arquivos Excel s\u00e3o lidos, e as colunas s\u00e3o ajustadas com base na 6\u00aa linha de cada arquivo.</p> </li> <li> <p>As linhas com dados ausentes na coluna <code>Dt Emiss\u00e3o</code> s\u00e3o removidas.</p> </li> <li> <p>Algumas colunas indesejadas s\u00e3o removidas, e o nome da coluna <code>Name</code> \u00e9 alterado para <code>Nome da Origem</code>.</p> </li> <li> <p>As colunas <code>MODAL</code> e <code>SKU</code> s\u00e3o derivadas a partir de outras colunas usando as fun\u00e7\u00f5es <code>determinar_modal</code> e <code>determinar_produto</code>.</p> </li> <li> <p>A coluna <code>Dt Emiss\u00e3o</code> \u00e9 convertida para o formato de data, e uma coluna de per\u00edodo mensal \u00e9 criada.</p> </li> </ul> </li> <li> <p>Filtragem de Dados: S\u00e3o filtrados apenas os dados relevantes, incluindo condi\u00e7\u00f5es espec\u00edficas para as colunas <code>Utiliza\u00e7\u00e3o</code>, <code>Status</code>, <code>UN</code>, e <code>Pre\u00e7o</code>.</p> </li> <li>C\u00e1lculo de Ticket M\u00e9dio: O ticket m\u00e9dio \u00e9 calculado por agrupamento de diversas colunas, e o valor \u00e9 arredondado para duas casas decimais.</li> <li>Cria\u00e7\u00e3o de Tabela Piv\u00f4: Uma tabela piv\u00f4 \u00e9 criada para reorganizar os dados por M\u00eas, COD_APRESENTACAO, SKU e Modal, com a m\u00e9dia do ticket m\u00e9dio.</li> <li>Limpeza de Nomes de Colunas: Os nomes das colunas na tabela piv\u00f4 s\u00e3o limpos utilizando a fun\u00e7\u00e3o <code>clean_column_name()</code>.</li> <li>Armazenamento no Banco de Dados: A tabela piv\u00f4 final \u00e9 armazenada no banco de dados de destino na tabela <code>fato_ticket_medio</code>.</li> </ol>"},{"location":"update_ticket_medio/#3-funcoes-auxiliares","title":"3. Fun\u00e7\u00f5es Auxiliares","text":"<ul> <li><code>determinar_modal</code>: Determina o modal com base nas informa\u00e7\u00f5es da coluna <code>Entidade</code>.</li> <li><code>determinar_produto</code>: Determina o SKU do produto com base nas informa\u00e7\u00f5es da coluna <code>Nome</code>.</li> <li><code>drop_views</code>: (Comentada no c\u00f3digo) Fun\u00e7\u00e3o que pode ser usada para descartar views no banco de dados, se necess\u00e1rio.</li> </ul>"},{"location":"update_ticket_medio/#transformacoes-realizadas-nos-dados","title":"Transforma\u00e7\u00f5es Realizadas nos Dados","text":"<ul> <li>Limpeza de Nomes de Colunas: Os nomes das colunas s\u00e3o normalizados para min\u00fasculas, acentos s\u00e3o removidos, e espa\u00e7os e pontos s\u00e3o substitu\u00eddos por underscores.</li> <li>Remo\u00e7\u00e3o de Colunas Indesejadas: Algumas colunas, como \"ICMS Orig\", \"IPI CST\", \"PIS\", entre outras, s\u00e3o removidas dos dados.</li> <li>C\u00e1lculos de Ticket M\u00e9dio: O pre\u00e7o \u00e9 agrupado por m\u00eas, COD_APRESENTACAO, SKU e MODAL, e o ticket m\u00e9dio \u00e9 calculado como a m\u00e9dia do pre\u00e7o.</li> <li>Tabela Piv\u00f4: A tabela piv\u00f4 \u00e9 criada para organizar os dados por M\u00eas, COD_APRESENTACAO, SKU e Modal, com os valores de ticket m\u00e9dio.</li> </ul>"},{"location":"update_ticket_medio/#dependencias","title":"Depend\u00eancias","text":"<p>O script depende das seguintes bibliotecas e m\u00f3dulos: - <code>os</code></p> <ul> <li> <p><code>pandas</code></p> </li> <li> <p><code>glob</code></p> </li> <li> <p><code>unidecode</code></p> </li> <li> <p><code>projeto_banco_cdd.modulos.sharepoint_downloader</code></p> </li> <li> <p><code>projeto_banco_cdd.modulos.functions_cdd</code></p> </li> </ul>"},{"location":"update_ticket_medio/#observacoes","title":"Observa\u00e7\u00f5es","text":"<ul> <li>O script baixa os arquivos Excel do SharePoint e processa apenas os arquivos que possuem a estrutura esperada.</li> <li>A fun\u00e7\u00e3o de c\u00e1lculo de ticket m\u00e9dio usa a m\u00e9dia do pre\u00e7o agrupado por M\u00eas, COD_APRESENTACAO, SKU e MODAL.</li> </ul>"},{"location":"views/","title":"Explica\u00e7\u00e3o das Views Criadas","text":""},{"location":"views/#1-view-extra_consolidado","title":"1. View: <code>extra_consolidado</code>","text":""},{"location":"views/#descricao","title":"Descri\u00e7\u00e3o","text":"<p>Esta view consolida os dados de vendas extras, agrupando as quantidades por data, c\u00f3digo de contrato (CT) e c\u00f3digo de apresenta\u00e7\u00e3o.</p>"},{"location":"views/#logica-de-negocio","title":"L\u00f3gica de Neg\u00f3cio","text":"<ul> <li>Soma as quantidades (<code>QTD</code>) das vendas extras por data, CT e apresenta\u00e7\u00e3o.</li> <li>Atribui um valor padr\u00e3o <code>99999</code> para <code>cod_ct</code> caso n\u00e3o esteja presente.</li> <li>Realiza um LEFT JOIN com as tabelas SCD de territ\u00f3rio para CT e GR, utilizando <code>cod_ct</code> e <code>cod_gr</code>, al\u00e9m do per\u00edodo correspondente. Dessa forma, soma apenas as quantidades vendidas enquanto os respectivos CTs e GRs eram respons\u00e1veis pelo territ\u00f3rio. Isso garante a unicidade dos dados por per\u00edodo, j\u00e1 que um mesmo territ\u00f3rio pode ter sido gerenciado por diferentes CTs e GRs em momentos distintos.</li> </ul>"},{"location":"views/#2-view-acesso_consolidado","title":"2. View: <code>acesso_consolidado</code>","text":""},{"location":"views/#descricao_1","title":"Descri\u00e7\u00e3o","text":"<p>Esta view agrupa os acessos por data, c\u00f3digo de CT e produto, separando as quantidades por modalidade (<code>Mercado Publico</code> e <code>Sa\u00fade Suplementar</code>).</p>"},{"location":"views/#logica-de-negocio_1","title":"L\u00f3gica de Neg\u00f3cio","text":"<ul> <li>Calcula a quantidade de frascos por modalidade.</li> <li>Realiza um LEFT JOIN com as tabelas SCD de territ\u00f3rio para CT e GR, utilizando <code>cod_ct</code> e <code>cod_gr</code>, al\u00e9m do per\u00edodo correspondente. Dessa forma, soma apenas as quantidades vendidas enquanto os respectivos CTs e GRs eram respons\u00e1veis pelo territ\u00f3rio. Isso garante a unicidade dos dados por per\u00edodo, j\u00e1 que um mesmo territ\u00f3rio pode ter sido gerenciado por diferentes CTs e GRs em momentos distintos.</li> </ul>"},{"location":"views/#3-view-vendas_consolidado","title":"3. View: <code>vendas_consolidado</code>","text":""},{"location":"views/#descricao_2","title":"Descri\u00e7\u00e3o","text":"<p>Consolida as vendas do CDD por m\u00eas, informante, apresenta\u00e7\u00e3o e ponto de venda.</p>"},{"location":"views/#logica-de-negocio_2","title":"L\u00f3gica de Neg\u00f3cio","text":"<ul> <li>Exclui vendas hospitalares (uma vez que o intuito dessa view \u00e9 computar vendas que ocorreram devido \u00e0 influ\u00eancia de CTs) e dos informantes <code>DIMED DISTR</code>, <code>LS</code>, <code>PORTAL</code>, <code>FARMACIAS ASSOCIADAS</code> (por serem redundantes).</li> <li>Converte unidades e valores para a escala correta (o CDD computa uma unidade com 1000).</li> <li>Realiza um LEFT JOIN com as tabelas SCD de territ\u00f3rio para CT e GR, utilizando <code>cod_ct</code> e <code>cod_gr</code>, al\u00e9m do per\u00edodo correspondente. Dessa forma, soma apenas as quantidades vendidas enquanto os respectivos CTs e GRs eram respons\u00e1veis pelo territ\u00f3rio. Isso garante a unicidade dos dados por per\u00edodo, j\u00e1 que um mesmo territ\u00f3rio pode ter sido gerenciado por diferentes CTs e GRs em momentos distintos.</li> </ul>"},{"location":"views/#4-view-pbm_consolidado","title":"4. View: <code>pbm_consolidado</code>","text":""},{"location":"views/#descricao_3","title":"Descri\u00e7\u00e3o","text":"<p>Consolida os dados de vendas do PBM (Programa de Benef\u00edcio em Medicamentos), considerando os descontos aplicados e ajustes na quantidade vendida.  </p>"},{"location":"views/#logica-de-negocio_3","title":"L\u00f3gica de Neg\u00f3cio","text":"<ul> <li>Filtra descontos superiores a 70%.  </li> <li>A partir de janeiro/2025, os descontos acima de 70% passaram a ser deduzidos das vendas do CT. No entanto, at\u00e9 dezembro/2024, apenas descontos \u2265 99% eram considerados. O filtro foi ajustado para refletir essa regra.  </li> <li> <p>A cl\u00e1usula SQL:  </p> <p><code>sql CASE WHEN fp.\"DESC ADM\" BETWEEN 70 AND 80 THEN \"QTDE\" * 0.67 ELSE \"QTDE\" END AS qtde</code></p> <p>Implementa a l\u00f3gica de aplica\u00e7\u00e3o do desconto de 70%, na qual, a cada tr\u00eas vouchers de 70%, duas vendas s\u00e3o descontadas. Isso resulta em um fator de ajuste de 2/3, substituindo a quantidade original por <code>\"QTDE\" * 0.67</code>, que representa a quantidade efetivamente reduzida no total de vendas do CT.  </p> </li> <li> <p>Realiza um LEFT JOIN com as tabelas SCD de territ\u00f3rio para CT e GR, utilizando <code>cod_ct</code>, <code>cod_gr</code> e o per\u00edodo correspondente. Dessa forma, as quantidades s\u00e3o somadas apenas durante os per\u00edodos em que os respectivos CTs e GRs eram respons\u00e1veis pelo territ\u00f3rio. Isso garante a unicidade dos dados por per\u00edodo, considerando que um mesmo territ\u00f3rio pode ter sido gerenciado por diferentes CTs e GRs em momentos distintos.  </p> </li> </ul>"},{"location":"views/#5-view-vw_sell_out","title":"5. View: <code>vw_sell_out</code>","text":""},{"location":"views/#descricao_4","title":"Descri\u00e7\u00e3o","text":"<p>Consolida todas as fontes de dados de vendas \u2014 <code>vendas_consolidado</code>, <code>extra_consolidado</code>, <code>acesso_consolidado</code> e <code>pbm_consolidado</code> \u2014 para oferecer uma vis\u00e3o unificada do sell-out. O resultado \u00e9 uma tabela que apresenta o n\u00famero de vendas, organizadas em colunas separadas para CDD, vendas extras, mercado p\u00fablico e sa\u00fade suplementar, segmentadas por dia, CT e GR. Essa tabela captura a dinamicidade das rela\u00e7\u00f5es entre territ\u00f3rio, CT e GR, sendo a principal fonte de dados para o dashboard de Metas e Resultados.  </p>"},{"location":"views/#logica-de-negocio_4","title":"L\u00f3gica de Neg\u00f3cio","text":"<ul> <li>Agrega os valores provenientes das diferentes origens de vendas.  </li> <li>Garante a coer\u00eancia na granularidade temporal e geogr\u00e1fica.  </li> <li>Relaciona-se com o ticket m\u00e9dio para o c\u00e1lculo dos valores das vendas.  </li> </ul>"},{"location":"views/#consideracoes-finais","title":"Considera\u00e7\u00f5es Finais","text":"<ul> <li>Todas as views asseguram a integridade dos dados atrav\u00e9s de joins com as tabelas de territ\u00f3rio e gerenciamento.</li> <li>Foram aplicadas regras de tratamento para casos nulos e valores padr\u00e3o (<code>99999</code> para <code>cod_ct</code> e <code>9999</code> para <code>cod_gr</code>).</li> <li>A estrutura visa otimizar consultas anal\u00edticas e fornecer uma vis\u00e3o confi\u00e1vel das vendas e acessos.</li> </ul>"}]}